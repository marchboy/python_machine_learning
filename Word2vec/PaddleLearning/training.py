# 词的向量表征，也称为word embedding.
# 词嵌入：把词映射为实数域上向量的技术也叫词嵌入（word embedding）。 https://blog.csdn.net/m0_37565948/article/details/84989565

# 为了比较两个词或者两段文本之间的相关性，我们往往先要把词表示成计算机适合处理的方式。
# 最自然的方式恐怕莫过于向量空间模型(vector space model)。 
# 在这种方式里，每个词被表示成一个实数向量（one-hot vector），
# 其长度为字典大小，每个维度对应一个字典里的每个词，除了这个词对应维度上的值是1，其他元素都是0。

# 词本身的信息量太少，导致两个词之间的相关性很低甚至是毫无相关性。
# 还需要从大量数据集里通过机器学习方法归纳出来更多信息。
# 词向量就是其中一种。
# 通过词向量模型可将一个one-hot vector映射到一个维度更低的实数向量（embedding vector），
# 如embedding(母亲节)=[0.3,4.2,−1.5,...],embedding(康乃馨)=[0.2,5.6,−2.3,...]。
# 在这个映射到的实数向量表示中，希望两个语义（或用法）上相似的词对应的词向量“更像”，
# 这样如“母亲节”和“康乃馨”的对应词向量的余弦相似度就不再为零了。

# 词向量模型可以是概率模型、共生矩阵(co-occurrence matrix)模型或神经元网络模型。
# 在用神经网络求词向量之前，传统做法是统计一个词语的共生矩阵X。
# X是一个|V| × |V|大小的矩阵，Xij表示在所有语料中，词汇表V(vocabulary)中第i个词和第j个词同时出现的词数，|V|为词汇表的大小。
# 对X做矩阵分解（如奇异值分解，Singular Value Decomposition [5]），得到的U即视为所有词的词向量，但这样的传统做法有很多问题：
# 1、由于很多词没有出现，导致矩阵极其稀疏，因此需要对词频做额外处理来达到更好的矩阵分解效果；
# 2、矩阵非常大，维度太高(通常达到106×106的数量级)；
# 3、需要手动去掉停用词（如although, a,...），不然这些频繁出现的词也会影响矩阵分解的效果。
# 基于神经网络的模型不需要计算和存储一个在全语料上统计产生的大表，而是通过学习语义信息得到词向量，因此能很好地解决以上问题。


# 特征值与特征向量 奇异值分解  A * x = lambda * x
# 表达了矩阵A的一个特性就是这个矩阵可以把向量x拉长（或缩短）lambda倍，仅此而已。

# 任意给定一个矩阵A，并不是对所有的向量x它都能拉长（缩短）。
# 凡是能被矩阵A拉长（缩短）的向量就称为矩阵A的特征向量（Eigenvector）；拉长（缩短）的量就是这个特征向量对应的特征值（Eigenvalue）。 
# 即如果矩阵对某一个（某些）向量只发生伸缩变换，不产生旋转效果，那么这个向量就称为这个矩阵的特征向量，伸缩比例就是特征值。

# 如果特征值是负数，则说明矩阵不但把特征向量拉长（缩短）了，而且使该向量的方向发生了反转（指向了相反的方向）。
# 一个矩阵可能可以拉长（缩短）多个向量，因此它就可能有多个特征值。另外，对于实对称矩阵来说，不同特征值对应的特征向量必定正交。




