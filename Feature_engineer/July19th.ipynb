{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "Some global parameters to be tuned here.\n",
    "\"\"\"\n",
    "date_range = (\"2016-01-01T00:00:00\", \"2016-03-31T23:59:59\")\n",
    "\n",
    "time_start = int(time.mktime(time.strptime(date_range[0], '%Y-%m-%dT%H:%M:%S')))\n",
    "time_end = int(time.mktime(time.strptime(date_range[1], '%Y-%m-%dT%H:%M:%S')))\n",
    "\n",
    "# TODO: def foo()\n",
    "# 7 days as a period\n",
    "period = 6047800.0\n",
    "n_period = int(math.ceil((time_end - time_start)/(period)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def time_str2int(in_time):\n",
    "    return int(time.mktime(time.strptime(in_time, '%Y-%m-%d %H:%M:%S'))) \n",
    "\n",
    "def feature_expenditure(filename, uuid_list):\n",
    "    \"\"\"\n",
    "    Input: filename-- expenditure_timeline\n",
    "           uuid_list-- uuids to be extracted from expenditure_timeline\n",
    "\n",
    "    Output: a dictionary of (uuid, feature) pairs\n",
    "\n",
    "    TODO: check possible repetitions of features\n",
    "    \"\"\"\n",
    "    e = pd.read_csv(filename, sep='\\t', header = None)\n",
    "    exp = e[e[0].isin(set(uuid_list))]\n",
    "\n",
    "    exp_dict = {}\n",
    "    for row in exp.iterrows():\n",
    "        uid = row[1][0]\n",
    "        \n",
    "        reg_df = string2list(row[1][1])\n",
    "        rec_df = string2list(row[1][2])\n",
    "        pay_df = string2list(row[1][3])\n",
    "        \n",
    "        temp = np.zeros(21)\n",
    "\n",
    "        # reg\n",
    "        if reg_df[3].any() == '0':\n",
    "            temp[0:6] = np.nan\n",
    "        else:\n",
    "            temp[0] = reg_df[1].nunique()\n",
    "            temp[1] = reg_df[2].nunique()\n",
    "            temp[2] = max(reg_df[3].map(time_str2int))\n",
    "            temp[3] = min(reg_df[3].map(time_str2int))\n",
    "            temp[4] = temp[2] - temp[3]\n",
    "            temp[5] = reg_df[5].nunique()\n",
    "\n",
    "        \n",
    "        # pay\n",
    "        if pay_df[2].any() == '0':\n",
    "            temp[6:13] = np.nan\n",
    "        else:\n",
    "            temp[6] = pay_df[0].nunique()\n",
    "            temp[7] = pay_df[1].nunique()\n",
    "            temp[8] = max(pay_df[2].map(time_str2int))\n",
    "            temp[9] = min(pay_df[2].map(time_str2int))\n",
    "            temp[10] = temp[8] - temp[9]\n",
    "            temp[11] = pay_df[4].nunique()\n",
    "            temp[12] = pay_df[5].nunique()\n",
    "\n",
    "        \n",
    "        # rec\n",
    "        if rec_df[2].any() == '0':\n",
    "            temp[13:21] = np.nan\n",
    "        else:\n",
    "            temp[13] = rec_df[0].nunique()\n",
    "            temp[14] = rec_df[1].nunique()\n",
    "            temp[15] = max(rec_df[2].map(time_str2int))\n",
    "            temp[16] = min(rec_df[2].map(time_str2int))\n",
    "            temp[17] = temp[15] - temp[16]\n",
    "            temp[18] = rec_df[3].nunique()\n",
    "            temp[19] = rec_df[4].nunique()\n",
    "            temp[20] = rec_df[6].nunique()\n",
    "\n",
    "        exp_dict[uid] = temp\n",
    "    return exp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_userbase(userbase_dataframe, uuid_list):\n",
    "    \"\"\"\n",
    "    Input: userbase_dataframe-- stacked userbases by pandas.read_csv()\n",
    "           uuid_list-- -- uuids to be extracted from userbase\n",
    "\n",
    "    Output: a dictionary of (uuid, feature) pairs\n",
    "    \"\"\"\n",
    "    u_by_uuid = userbase_dataframe[userbase_dataframe[1].isin(set(uuid_list))]\n",
    "\n",
    "    # remove empty uuids: 'cfcd208495d565ef66e7dff9f98764da' \n",
    "    u_by_uuid = u_by_uuid[u_by_uuid[1] != 'cfcd208495d565ef66e7dff9f98764da']\n",
    "\n",
    "    grouped_by_u = u_by_uuid.groupby([1])\n",
    "    \n",
    "    userbase_dict = {}\n",
    "    for each in grouped_by_u.groups:\n",
    "        \n",
    "        index_list = grouped_by_u.groups[each]\n",
    "        data_frame = u_by_uuid.loc[index_list]\n",
    "        \n",
    "        try:\n",
    "            # uid\n",
    "            temp = [len(index_list)]\n",
    "\n",
    "            # reg_ip\n",
    "            temp.append(data_frame[2].nunique())\n",
    "\n",
    "            # signature\n",
    "            if sum(data_frame[3].isnull()) > 0:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "\n",
    "            # nickname\n",
    "            #nn = data_frame['4'].map(lambda x: x == 'None')\n",
    "            if sum(data_frame[4].map(lambda x: x == 'None')) > 0:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "\n",
    "            # sex majority\n",
    "            temp.append(str(data_frame[5].value_counts().index[0]))\n",
    "            # sex unique count\n",
    "            temp.append(data_frame[5].nunique())\n",
    "\n",
    "            # platform majority\n",
    "            temp.append(str(data_frame[6].value_counts().index[0]))\n",
    "            # platform unique count\n",
    "            temp.append(data_frame[6].nunique())\n",
    "\n",
    "            # ucid majority\n",
    "            temp.append(str(data_frame[8].value_counts().index[0]))\n",
    "            # ucid unique count\n",
    "            temp.append(data_frame[8].nunique())\n",
    "\n",
    "            # reg time max\n",
    "            temp.append(max(data_frame[9].map(time_str2int)))\n",
    "            # reg time min\n",
    "            temp.append(min(data_frame[9].map(time_str2int)))\n",
    "            # reg time span\n",
    "            temp.append(max(data_frame[9].map(time_str2int)) - min(data_frame[9].map(time_str2int)))\n",
    "\n",
    "            # group: 1, 0\n",
    "            if sum(data_frame[10].map(lambda x: x != '[]')) > 0:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "            # group: number of groups\n",
    "            temp.append(len(','.join(data_frame[10]).split(',')))\n",
    "\n",
    "            # name\n",
    "            if sum(data_frame[11].map(lambda x: x != 'None')) > 0:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "\n",
    "            if sum(data_frame[12]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)\n",
    "\n",
    "            if sum(data_frame[13]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)\n",
    "\n",
    "            if sum(data_frame[14]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)\n",
    "\n",
    "            if sum(data_frame[15]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)\n",
    "\n",
    "            if sum(data_frame[16]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)    \n",
    "\n",
    "            if sum(data_frame[17]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)\n",
    "\n",
    "            if sum(data_frame[18]) > 0:\n",
    "                temp.append(1)\n",
    "            else: \n",
    "                temp.append(0)\n",
    "\n",
    "            userbase_dict[each] = temp\n",
    "\n",
    "        except:\n",
    "            print data_frame\n",
    "            del data_frame\n",
    "            continue\n",
    "\n",
    "    return userbase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_event(filename):\n",
    "\n",
    "    # Get logs within the time range\n",
    "    t = pd.read_csv(filename, sep='\\t', header = None)\n",
    "    t1 = t[t[2] >= time_start]\n",
    "    data_all = t1[t1[2] <= time_end]\n",
    "\n",
    "    # Get all unique event ids\n",
    "    unique_event_ids = data_all[1].unique()\n",
    "\n",
    "    grouped_by_uuid = data_all.groupby([0])\n",
    "    uuids = grouped_by_uuid.groups.keys()\n",
    "\n",
    "    # binning whole logs in periods as a big dict()\n",
    "    whole_dict = {i : {} for i in range(n_period)}\n",
    "    # for each log\n",
    "    for log in data_all.iterrows():\n",
    "        # find its binned period\n",
    "        idx = int(math.floor((log[1][2] - time_start)/period))\n",
    "\n",
    "        if log[1][0] not in whole_dict[idx]:\n",
    "            whole_dict[idx][log[1][0]] = []\n",
    "\n",
    "        # append event id and timestamp to the list\n",
    "        whole_dict[idx][log[1][0]].append((log[1][1], log[1][2]))\n",
    "\n",
    "    return whole_dict\n",
    "\n",
    "def get_labels(whole_dict):\n",
    "\n",
    "    # creating labels for churn = 1, stay = 0\n",
    "    labels = {i : {} for i in range(n_period)}\n",
    "    #for each period\n",
    "    for i in range(n_period - 1):\n",
    "        # for each uuid\n",
    "        for uuid in whole_dict[i]:\n",
    "            # check if the uuid appears in the next period, if yes-> stay; no-> churn\n",
    "            if uuid in whole_dict[i + 1]:\n",
    "                labels[i][uuid] = 0\n",
    "            else:\n",
    "                labels[i][uuid] = 1\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uuids(filename):\n",
    "    # Get logs within the time range\n",
    "    t = pd.read_csv(filename, sep='\\t', header = None)\n",
    "    t1 = t[t[2] >= time_start]\n",
    "    data_all = t1[t1[2] <= time_end]\n",
    "\n",
    "    uuids = data_all[0].unique()\n",
    "\n",
    "    return uuids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# get event dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_dict = feature_event(\"timeline/timeline_event_gpapp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uuid_list = get_uuids(\"timeline/timeline_event_gpapp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(whole_dict, \"whole_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get userbase dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (3,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "u0 = pd.read_csv(\"timeline/user_base_0.txt\", sep='\\t', header = None)\n",
    "u1 = pd.read_csv(\"timeline/user_base_1.txt\", sep='\\t', header = None)\n",
    "u2 = pd.read_csv(\"timeline/user_base_2.txt\", sep='\\t', header = None)\n",
    "u3 = pd.read_csv(\"timeline/user_base_3.txt\", sep='\\t', header = None)\n",
    "u4 = pd.read_csv(\"timeline/user_base_4.txt\", sep='\\t', header = None)\n",
    "u5 = pd.read_csv(\"timeline/user_base_5.txt\", sep='\\t', header = None)\n",
    "u6 = pd.read_csv(\"timeline/user_base_6.txt\", sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = u0.append(u1).append(u2).append(u3).append(u4).append(u5).append(u6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      0                                 1   \\\n",
      "240245  0252592529ded94f52ee57bc143b3b6d  34d253f365546356dad11d3fba91ab21   \n",
      "\n",
      "                 2    3    4    5    6    7    8    9    10   11  12  13  14  \\\n",
      "240245  58.46.41.39  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN NaN   \n",
      "\n",
      "        15  16  17  18  \n",
      "240245 NaN NaN NaN NaN  \n",
      "                                      0                                 1   \\\n",
      "115131  1b86c7940a229b6f7defd5b0058b4a21  6042e177e50a64a42eec091cddd617d9   \n",
      "115131  0ce036e92175501804587603fe81140e  4d40aa7f2f836307d29ad524f85beaaa   \n",
      "\n",
      "                    2          3     4     5    6    7    8   \\\n",
      "115131    36.45.13.103        NaN  None  None  102    0  144   \n",
      "115131  113.93.111.136  无敌是我，我是无敌   NaN   NaN  NaN  NaN  NaN   \n",
      "\n",
      "                         9    10         11   12   13   14   15   16   17   18  \n",
      "115131  2016-03-27 21:30:22   []  gp7895353  1.0  1.0  1.0  1.0  1.0  0.0  1.0  \n",
      "115131                  NaN  NaN        NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "                                      0                                 1   \\\n",
      "115131  1b86c7940a229b6f7defd5b0058b4a21  6042e177e50a64a42eec091cddd617d9   \n",
      "115131  0ce036e92175501804587603fe81140e  4d40aa7f2f836307d29ad524f85beaaa   \n",
      "\n",
      "                    2          3     4     5    6    7    8   \\\n",
      "115131    36.45.13.103        NaN  None  None  102    0  144   \n",
      "115131  113.93.111.136  无敌是我，我是无敌   NaN   NaN  NaN  NaN  NaN   \n",
      "\n",
      "                         9    10         11   12   13   14   15   16   17   18  \n",
      "115131  2016-03-27 21:30:22   []  gp7895353  1.0  1.0  1.0  1.0  1.0  0.0  1.0  \n",
      "115131                  NaN  NaN        NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "                                      0                                 1   \\\n",
      "187197  56899232b406a3fb3ad1eee9ee5d1191  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "187197  fe163c205f88c88bd83c9eeefb5cf5f7  fb676ab5d8a34b663b8ff588e83b3cd4   \n",
      "\n",
      "                    2      3     4     5    6    7    8                    9   \\\n",
      "187197  183.19.132.209    NaN  None  None  102    0  100  2016-02-18 22:57:07   \n",
      "187197  153.99.190.153  一起睡觉吧   NaN   NaN  NaN  NaN  NaN                  NaN   \n",
      "\n",
      "         10            11   12   13   14   15   16   17   18  \n",
      "187197   []  gp3211367003  1.0  1.0  1.0  1.0  1.0  0.0  1.0  \n",
      "187197  NaN           NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "                                      0                                 1   \\\n",
      "63612   e0d4c4c313e8120bd76997a95ac1bd46  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "17959   4ff1e70e39acd4b84daacd48b8c1ec46  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "454664  e3b697037f739353b3251d10cc3e730f  5c5a0747eeb268abcf758a39a31c110e   \n",
      "454664  3a5031aacd819bf2c91815db874521c9  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "20881   f456f571c8918dda4229d3e0fbb8465b  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "187197  56899232b406a3fb3ad1eee9ee5d1191  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "187197  fe163c205f88c88bd83c9eeefb5cf5f7  fb676ab5d8a34b663b8ff588e83b3cd4   \n",
      "364011  cf15fe7669e3e665add58036eacbd923  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "357297  ae48ecc325735f5ba1a799fabdf49822  ef2fdf5f2fd892df4f2289ddef3d6e54   \n",
      "357297  21b3b77963c6d02e16253490fa828ce7  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "478278  b74078cbd7e38ca289386a2045c4dfae  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "443984  24b2332565cbab33cb373d7f572cbb11  b1e265e83c69f5182d1c22c8fc7dd475   \n",
      "\n",
      "                     2      3         4     5    6    7     8   \\\n",
      "63612    183.19.154.125    NaN      None  None  102    0  9689   \n",
      "17959    183.19.154.125    NaN      None  None  102    0  9689   \n",
      "454664   58.211.159.150    NaN  振奋27的小雨儿  None  102    0  5241   \n",
      "454664   183.19.154.125    NaN      None  None  102    0  9689   \n",
      "20881        14.30.0.49    NaN      None  None  102    0  1891   \n",
      "187197   183.19.132.209    NaN      None  None  102    0   100   \n",
      "187197   153.99.190.153  一起睡觉吧       NaN   NaN  NaN  NaN   NaN   \n",
      "364011   125.88.122.103    NaN      None  None  102    0  9689   \n",
      "357297  117.136.101.109    NaN    幸福7的秋葵  None  102    0   100   \n",
      "357297   183.19.154.125    NaN      None  None  102    0  9689   \n",
      "478278       14.30.0.49    NaN      None  None  102    0  1891   \n",
      "443984   125.88.122.103    NaN      None  None  102    0  9689   \n",
      "\n",
      "                         9    10              11   12   13   14   15   16  \\\n",
      "63612   2016-03-08 11:08:55   []    gp3211367001  1.0  1.0  1.0  1.0  1.0   \n",
      "17959   2016-03-08 01:05:20   []    gp3211367004  1.0  1.0  1.0  1.0  1.0   \n",
      "454664  2016-02-17 08:28:52   []       gp6060237  1.0  1.0  1.0  1.0  1.0   \n",
      "454664  2016-03-08 14:08:55   []    gp3211367llp  1.0  1.0  1.0  1.0  1.0   \n",
      "20881   2016-01-02 11:04:33   []  gp321136728101  1.0  1.0  1.0  1.0  1.0   \n",
      "187197  2016-02-18 22:57:07   []    gp3211367003  1.0  1.0  1.0  1.0  1.0   \n",
      "187197                  NaN  NaN             NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "364011  2016-03-08 17:44:07   []    gp3211367007  1.0  1.0  1.0  1.0  1.0   \n",
      "357297  2016-02-07 19:14:59   []       gp5366578  1.0  1.0  1.0  1.0  1.0   \n",
      "357297  2016-03-08 09:52:44   []    gp3211367005  1.0  1.0  1.0  1.0  1.0   \n",
      "478278  2016-01-02 10:05:15   []  gp321136728201  1.0  1.0  1.0  1.0  1.0   \n",
      "443984  2016-03-08 16:37:34   []    gp3211367006  1.0  1.0  1.0  1.0  1.0   \n",
      "\n",
      "         17   18  \n",
      "63612   0.0  1.0  \n",
      "17959   0.0  1.0  \n",
      "454664  0.0  1.0  \n",
      "454664  0.0  1.0  \n",
      "20881   0.0  1.0  \n",
      "187197  0.0  1.0  \n",
      "187197  NaN  NaN  \n",
      "364011  0.0  1.0  \n",
      "357297  0.0  1.0  \n",
      "357297  0.0  1.0  \n",
      "478278  0.0  1.0  \n",
      "443984  0.0  1.0  \n"
     ]
    }
   ],
   "source": [
    "userbase_dict = feature_userbase(u, uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(userbase_dict, \"userbase_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uid_list = uid_list[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115340"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get expenditure dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uids(userbase_dataframe, uuid_list):\n",
    "    # TODO\n",
    "    u_by_uuid = userbase_dataframe[userbase_dataframe[1].isin(set(uuid_list))]\n",
    "    u_by_uuid = u_by_uuid[u_by_uuid[1]!= 'cfcd208495d565ef66e7dff9f98764da']\n",
    "    return u_by_uuid[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uid_list = get_uids(u, uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string2list(in_str):\n",
    "    ret = in_str.split('],[')\n",
    "    temp = []\n",
    "    for each in ret:\n",
    "        temp.append(each.strip('[').strip(']').split(','))\n",
    "    return pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_dict = feature_expenditure(\"C:/Users/Administrator/timeline/expenditure_timeline.txt\", uid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_obj(exp_dict, \"expenditure_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115340"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# start with whole_dict, exp_dict, userbase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whole_dict = load_obj('whole_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_dict = load_obj('expenditure_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userbase_dict = load_obj('userbase_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = get_labels(whole_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eventids(filename):\n",
    "    # Get logs within the time range\n",
    "    t = pd.read_csv(filename, sep='\\t', header = None)\n",
    "    t1 = t[t[2] >= time_start]\n",
    "    data_all = t1[t1[2] <= time_end]\n",
    "\n",
    "    grouped_by_eventid = data_all.groupby([1])\n",
    "    eventids = grouped_by_eventid.groups.keys()\n",
    "    return eventids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eventids = get_eventids('timeline/timeline_event_gpapp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# userbase feature v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_filter(input_element, filter_list, replacement):\n",
    "    if input_element not in filter_list:\n",
    "        return replacement\n",
    "    return input_element\n",
    "\n",
    "def ucid_cleaner(input_element):\n",
    "    try:\n",
    "        ret = eval(input_element)\n",
    "    except:\n",
    "        ret = []\n",
    "    if type(ret) is list:\n",
    "        return ret\n",
    "    elif type(ret) is dict:\n",
    "        return map(lambda x: int(x), ret.values())\n",
    "    elif type(ret) is int:\n",
    "        return [ret]\n",
    "    else:\n",
    "        print \"Error in ucid_cleaner\"\n",
    "\n",
    "def feature_userbase(userbase_dataframe, uuid_list):\n",
    "    \"\"\"\n",
    "    Input: userbase_dataframe-- stacked userbases by pandas.read_csv()\n",
    "           uuid_list-- -- uuids to be extracted from userbase\n",
    "\n",
    "    Output: a dictionary of (uuid, feature) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    u_by_uuid = userbase_dataframe[userbase_dataframe[1].isin(set(uuid_list))]\n",
    "    # remove empty uuids: 'cfcd208495d565ef66e7dff9f98764da' \n",
    "    u_by_uuid = u_by_uuid[u_by_uuid[1] != 'cfcd208495d565ef66e7dff9f98764da']\n",
    "    # remove NaNs\n",
    "    u_by_uuid.drop(u_by_uuid[u_by_uuid[9].isnull()].index, inplace=True)\n",
    "\n",
    "    \"\"\"Some data cleaning\"\"\"\n",
    "    # clean sex\n",
    "    value_counts = u_by_uuid[5].value_counts()\n",
    "    u_by_uuid[5] = u_by_uuid[5].map(lambda x: cleaning_filter(x, value_counts.index[:4], value_counts.index[0]))\n",
    "\n",
    "    # clean platform\n",
    "    u_by_uuid[6] = u_by_uuid[6].map(lambda x: str(x))\n",
    "    value_counts = u_by_uuid[6].value_counts()\n",
    "    u_by_uuid[6] = u_by_uuid[6].map(lambda x: cleaning_filter(x, value_counts.index[:3], '0'))\n",
    "\n",
    "    # clean status\n",
    "    u_by_uuid[7] = u_by_uuid[7].map(lambda x: str(x))\n",
    "    u_by_uuid[7] = u_by_uuid[7].map(lambda x: cleaning_filter(x, ['0'], '1'))\n",
    "\n",
    "    # clean ucid\n",
    "    u_by_uuid[8] = u_by_uuid[8].map(lambda x: str(x))\n",
    "\n",
    "    \"\"\"some processing about ucid\"\"\"\n",
    "    # clean ucid\n",
    "    u_by_uuid[10] = u_by_uuid[10].map(ucid_cleaner)\n",
    "    ucids = []\n",
    "    for each in u_by_uuid[10]:\n",
    "        ucids += each\n",
    "    dictinct_ucids = list(set(ucids))\n",
    "\n",
    "    # Begin feature engineering\n",
    "    grouped_by_u = u_by_uuid.groupby([1])\n",
    "    \n",
    "    userbase_dict = {}\n",
    "    for each in grouped_by_u.groups:\n",
    "        \n",
    "        index_list = grouped_by_u.groups[each]\n",
    "        data_frame = u_by_uuid.loc[index_list]\n",
    "        \n",
    "#         try:\n",
    "        #[0] uid\n",
    "        temp = [len(index_list)]\n",
    "\n",
    "        #[1] reg_ip\n",
    "        temp.append(data_frame[2].nunique())\n",
    "\n",
    "        #[2] has signature or not\n",
    "        if sum(data_frame[3].isnull()) > 0:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "\n",
    "        #[3] has nickname or not\n",
    "        #nn = data_frame['4'].map(lambda x: x == 'None')\n",
    "        if sum(data_frame[4].map(lambda x: x == 'None')) > 0:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "\n",
    "        #[4] sex majority -- One-Hot needed\n",
    "        temp.append(str(data_frame[5].value_counts().index[0]))\n",
    "        #[5] sex unique count\n",
    "        temp.append(data_frame[5].nunique())\n",
    "\n",
    "        #[6] platform majority -- One-Hot needed\n",
    "        temp.append(str(data_frame[6].value_counts().index[0]))\n",
    "        #[7] platform unique count\n",
    "        temp.append(data_frame[6].nunique())\n",
    "\n",
    "        #[8] ucid majority\n",
    "        temp.append(str(data_frame[8].value_counts().index[0]))\n",
    "        #[9] ucid unique count\n",
    "        temp.append(data_frame[8].nunique())\n",
    "\n",
    "        #[10] reg time max\n",
    "        temp.append(max(data_frame[9].map(time_str2int)))\n",
    "        #[11] reg time min\n",
    "        temp.append(min(data_frame[9].map(time_str2int)))\n",
    "        #[12] reg time span\n",
    "        temp.append(max(data_frame[9].map(time_str2int)) - min(data_frame[9].map(time_str2int)))\n",
    "        #[13] reg time mean\n",
    "        temp.append(data_frame[9].map(time_str2int).quantile(0.5))\n",
    "        #[14] reg time std\n",
    "        temp.append(np.std(data_frame[9].map(time_str2int)))\n",
    "\n",
    "\n",
    "        #[15] group: number of groups\n",
    "        temp.append( sum(data_frame[10].map(lambda x: len(x))) )\n",
    "        #[16] group: dummy\n",
    "        temp.append(int(temp[len(temp)-1] > 0))\n",
    "\n",
    "        \"\"\"TODO: a huge feature line of ucid down in the bottom\"\"\"\n",
    "\n",
    "        #[18] name\n",
    "        if sum(data_frame[11].map(lambda x: x != 'None')) > 0:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "\n",
    "        #[19-25]\n",
    "        if sum(data_frame[12]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)\n",
    "\n",
    "        if sum(data_frame[13]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)\n",
    "\n",
    "        if sum(data_frame[14]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)\n",
    "\n",
    "        if sum(data_frame[15]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)\n",
    "\n",
    "        if sum(data_frame[16]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)    \n",
    "\n",
    "        if sum(data_frame[17]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)\n",
    "\n",
    "        if sum(data_frame[18]) > 0:\n",
    "            temp.append(1)\n",
    "        else: \n",
    "            temp.append(0)\n",
    "\n",
    "\n",
    "        \"\"\"the huge line promised above\"\"\"\n",
    "        df_ucids = []\n",
    "        for ucids_list in data_frame[10]:\n",
    "            df_ucids += ucids_list\n",
    "        ucid_feature = [0] * len(dictinct_ucids)\n",
    "        for i, e in enumerate(dictinct_ucids):\n",
    "            if e in df_ucids:\n",
    "                ucid_feature[i] += 1\n",
    "\n",
    "        temp += ucid_feature\n",
    "\n",
    "        userbase_dict[each] = temp\n",
    "\n",
    "#         except:\n",
    "#             e = sys.exc_info()[0]\n",
    "#             print data_frame\n",
    "#             print e\n",
    "#             break\n",
    "        #    del data_frame\n",
    "        #    continue\n",
    "\n",
    "    return userbase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ub_feature = feature_userbase(u, uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_obj(ub_feature, \"userbase_dict_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ub_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8effc629e4789156b85646698b27f943\n"
     ]
    }
   ],
   "source": [
    "print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ucid_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f911c119de7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mucid_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ucid_feature' is not defined"
     ]
    }
   ],
   "source": [
    "ucid_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8effc629e4789156b85646698b27f943': [1, 2, 3]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = {}\n",
    "hi[each] = [1, 2, 3]\n",
    "hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    u_by_uuid = u[u[1].isin(set(uuid_list))]\n",
    "    # remove empty uuids: 'cfcd208495d565ef66e7dff9f98764da' \n",
    "    u_by_uuid = u_by_uuid[u_by_uuid[1] != 'cfcd208495d565ef66e7dff9f98764da']\n",
    "    # remove NaNs\n",
    "    u_by_uuid.drop(u_by_uuid[u_by_uuid[9].isnull()].index, inplace=True)\n",
    "\n",
    "    \"\"\"Some data cleaning\"\"\"\n",
    "    # clean sex\n",
    "    value_counts = u_by_uuid[5].value_counts()\n",
    "    u_by_uuid[5] = u_by_uuid[5].map(lambda x: cleaning_filter(x, value_counts.index[:4], value_counts.index[0]))\n",
    "\n",
    "    # clean platform\n",
    "    u_by_uuid[6] = u_by_uuid[6].map(lambda x: str(x))\n",
    "    value_counts = u_by_uuid[6].value_counts()\n",
    "    u_by_uuid[6] = u_by_uuid[6].map(lambda x: cleaning_filter(x, value_counts.index[:3], '0'))\n",
    "\n",
    "    # clean status\n",
    "    u_by_uuid[7] = u_by_uuid[7].map(lambda x: str(x))\n",
    "    u_by_uuid[7] = u_by_uuid[7].map(lambda x: cleaning_filter(x, ['0'], '1'))\n",
    "\n",
    "    # clean ucid\n",
    "    u_by_uuid[8] = u_by_uuid[8].map(lambda x: str(x))\n",
    "\n",
    "    \"\"\"some processing about ucid\"\"\"\n",
    "    # clean ucid\n",
    "    u_by_uuid[10] = u_by_uuid[10].map(ucid_cleaner)\n",
    "    ucids = []\n",
    "    for each in u_by_uuid[10]:\n",
    "        ucids += each\n",
    "    dictinct_ucids = list(set(ucids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictinct_ucids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    grouped_by_u = u_by_uuid.groupby([1])\n",
    "    \n",
    "    for each in grouped_by_u.groups:\n",
    "        \n",
    "        index_list = grouped_by_u.groups[each]\n",
    "        data_frame = u_by_uuid.loc[index_list]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137818    []\n",
       "111268    []\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ucid_feature = [0] * len(dictinct_ucids)\n",
    "for i, e in enumerate(dictinct_ucids):\n",
    "    if e in data_frame[10]:\n",
    "        ucid_feature[i] += 1\n",
    "\n",
    "temp += ucid_feature\n",
    "\n",
    "userbase_dict[each] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# expenditure v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uuid_list = get_uuids(\"timeline/timeline_event_gpapp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uuid2uid(userbase_dataframe, uuid_list):\n",
    "    # TODO\n",
    "    u_by_uuid = userbase_dataframe[userbase_dataframe[1].isin(set(uuid_list))]\n",
    "    u_by_uuid = u_by_uuid[u_by_uuid[1]!= 'cfcd208495d565ef66e7dff9f98764da']\n",
    "    uuid2uid = {}\n",
    "    for each in u_by_uuid.iterrows():\n",
    "        if each[1][1] not in uuid2uid:\n",
    "            uuid2uid[each[1][1]] = []\n",
    "        uuid2uid[each[1][1]].append(each[1][0])\n",
    "    return uuid2uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uuid2uid = get_uuid2uid(u, uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_by_uuid = u[u[1].isin(set(uuid_list))]\n",
    "u_by_uuid = u_by_uuid[u_by_uuid[1]!= 'cfcd208495d565ef66e7dff9f98764da']\n",
    "uid2uuid = {}\n",
    "for each in u_by_uuid.iterrows():\n",
    "    if each[1][0] not in uid2uuid:\n",
    "        uid2uuid[each[1][0]] = []\n",
    "    uid2uuid[each[1][0]].append(each[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45122"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uuid2uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115340"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid2uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_expenditure(filename, uid_list, uuid2uid):\n",
    "    \"\"\"\n",
    "    Input: filename-- expenditure_timeline\n",
    "           uid_list-- uids to be extracted from expenditure_timeline\n",
    "\n",
    "    Output: a dictionary of (uuid, feature) pairs\n",
    "\n",
    "    TODO: check possible repetitions of features\n",
    "    \"\"\"\n",
    "    e = pd.read_csv(filename, sep='\\t', header = None)\n",
    "    exp = e[e[0].isin(set(uid_list))]\n",
    "\n",
    "    exp_dict = {}\n",
    "    for row in exp.iterrows():\n",
    "        uid = row[1][0]\n",
    "        \n",
    "        reg_df = string2list(row[1][1])\n",
    "        rec_df = string2list(row[1][2])\n",
    "        pay_df = string2list(row[1][3])\n",
    "        \n",
    "        temp = np.zeros(21)\n",
    "\n",
    "        # reg\n",
    "        if reg_df[3].any() == '0':\n",
    "            temp[0:6] = np.nan\n",
    "        else:\n",
    "            temp[0] = reg_df[1].nunique()\n",
    "            temp[1] = reg_df[2].nunique()\n",
    "            temp[2] = max(reg_df[3].map(time_str2int))\n",
    "            temp[3] = min(reg_df[3].map(time_str2int))\n",
    "            temp[4] = temp[2] - temp[3]\n",
    "            temp[5] = reg_df[5].nunique()\n",
    "\n",
    "        \n",
    "        # pay\n",
    "        if pay_df[2].any() == '0':\n",
    "            temp[6:13] = np.nan\n",
    "        else:\n",
    "            temp[6] = pay_df[0].nunique()\n",
    "            temp[7] = pay_df[1].nunique()\n",
    "            temp[8] = max(pay_df[2].map(time_str2int))\n",
    "            temp[9] = min(pay_df[2].map(time_str2int))\n",
    "            temp[10] = temp[8] - temp[9]\n",
    "            temp[11] = pay_df[4].nunique()\n",
    "            temp[12] = pay_df[5].nunique()\n",
    "\n",
    "        \n",
    "        # rec\n",
    "        if rec_df[2].any() == '0':\n",
    "            temp[13:21] = np.nan\n",
    "        else:\n",
    "            temp[13] = rec_df[0].nunique()\n",
    "            temp[14] = rec_df[1].nunique()\n",
    "            temp[15] = max(rec_df[2].map(time_str2int))\n",
    "            temp[16] = min(rec_df[2].map(time_str2int))\n",
    "            temp[17] = temp[15] - temp[16]\n",
    "            temp[18] = rec_df[3].nunique()\n",
    "            temp[19] = rec_df[4].nunique()\n",
    "            temp[20] = rec_df[6].nunique()\n",
    "\n",
    "        exp_dict[uid] = temp\n",
    "\n",
    "    return exp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115340"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid2uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = pd.read_csv('timeline/expenditure_timeline.txt', sep='\\t', header = None)\n",
    "exp = e[e[0].isin(set(uid2uuid.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "exp[-1] = exp[0].map(lambda x: uid2uuid[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[航海王强者之路,512,2016-03-16 23:00:44,100,金币,101147,0],[航海王强者之路,512,2016-03-31 15:37:52,144,金币,101147,0],[航海王强者之路,512,2016-04-01 08:12:13,144,金币,101147,0]\n"
     ]
    }
   ],
   "source": [
    "print exp[2][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_by_uuid = exp.groupby([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_by_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>航海王强者之路</td>\n",
       "      <td>101147</td>\n",
       "      <td>100</td>\n",
       "      <td>2016-02-26 14:44:13</td>\n",
       "      <td>ZS4ADN4UDRE8XVE5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>部落冲突-2</td>\n",
       "      <td>101728</td>\n",
       "      <td>100</td>\n",
       "      <td>2016-02-26 10:25:13</td>\n",
       "      <td>1803607402</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1    2                    3                 4    5\n",
       "0  航海王强者之路  101147  100  2016-02-26 14:44:13  ZS4ADN4UDRE8XVE5  100\n",
       "1   部落冲突-2  101728  100  2016-02-26 10:25:13        1803607402  100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string2list(exp[1][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115340"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ret = {}\n",
    "for each in exp_by_uuid.groups:\n",
    "    index_list = exp_by_uuid.groups[each]\n",
    "    data_frame = exp.loc[index_list]\n",
    "    \n",
    "    reg = []\n",
    "    rec = []\n",
    "    pay = []\n",
    "    for row in data_frame.iterrows():\n",
    "        reg += string2list(row[1][1])\n",
    "        rec += string2list(row[1][2])\n",
    "        pay += string2list(row[1][3])\n",
    "    reg = pd.DataFrame(reg)\n",
    "    rec = pd.DataFrame(rec)\n",
    "    pay = pd.DataFrame(pay)\n",
    "    \n",
    "    \n",
    "    \"\"\"reg\"\"\"\n",
    "    reg = reg[reg[3]!='0']\n",
    "    try:\n",
    "        #[0] #uid\n",
    "        temp = [len(index_list)]\n",
    "        #[1] #games\n",
    "        temp.append(len(reg[0]))\n",
    "        #[2] # unique games\n",
    "        temp.append(len(reg[1].unique()))\n",
    "        #[3] # unique cid\n",
    "        temp.append(len(reg[2].unique()))\n",
    "        #[4] # unique game_uid\n",
    "        temp.append(len(reg[4].unique()))\n",
    "        #[5] # unique ucid\n",
    "        temp.append(len(reg[5].unique()))\n",
    "\n",
    "        reg_time = reg[3].map(time_str2int)\n",
    "        #[6] max reg time\n",
    "        temp.append(max(reg_time))\n",
    "        #[7] min reg time\n",
    "        temp.append(min(reg_time))\n",
    "        #[8] reg time span\n",
    "        temp.append(max(reg_time) - min(reg_time))\n",
    "        #[9] reg time mean\n",
    "        temp.append(reg_time.quantile(0.5))\n",
    "        #[10] reg time std\n",
    "        temp.append(np.std(reg_time))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    \"\"\"rec\"\"\"\n",
    "    rec = rec[rec[2]!='0']\n",
    "    if len(rec) == 0:\n",
    "        temp += [np.nan] * 12\n",
    "    else:\n",
    "        #[11] # rec\n",
    "        temp.append(len(rec[0]))\n",
    "        #[12] # unique games\n",
    "        temp.append(len(rec[0].unique()))\n",
    "        #[13] # unique types\n",
    "        temp.append(len(rec[1].unique()))\n",
    "        #[14] # unique cid\n",
    "        temp.append(len(rec[3].unique()))\n",
    "        #[15] # unique item\n",
    "        temp.append(len(rec[4].unique()))    \n",
    "        #[16] # unique appid\n",
    "        temp.append(len(rec[5].unique()))  \n",
    "        #[17] # unique pay_from\n",
    "        temp.append(len(rec[6].unique()))  \n",
    "\n",
    "        rec_time = rec[2].map(time_str2int)\n",
    "        #[18] max reg time\n",
    "        temp.append(max(rec_time))\n",
    "        #[19] min reg time\n",
    "        temp.append(min(rec_time))\n",
    "        #[20] reg time span\n",
    "        temp.append(max(rec_time) - min(rec_time))\n",
    "        #[21] reg time mean\n",
    "        temp.append(rec_time.quantile(0.5))\n",
    "        #[22] reg time std\n",
    "        temp.append(np.std(rec_time))\n",
    "    \n",
    "    pay = pay[pay[2]!='0']\n",
    "    if len(pay) == 0:\n",
    "        temp += [np.nan] * 11\n",
    "    else:\n",
    "        #[23] # payments\n",
    "        temp.append(len(pay[0]))\n",
    "        #[24] # unique games\n",
    "        temp.append(len(pay[0].unique()))\n",
    "        #[25] # unique cid\n",
    "        temp.append(len(pay[1].unique()))\n",
    "        #[26] # unique appid\n",
    "        temp.append(len(pay[3].unique()))\n",
    "        #[27] # unique item\n",
    "        temp.append(len(pay[4].unique()))    \n",
    "        #[28] # unique porder\n",
    "        temp.append(len(pay[5].unique()))  \n",
    "\n",
    "\n",
    "        pay_time = pay[2].map(time_str2int)\n",
    "        #[29] max reg time\n",
    "        temp.append(max(pay_time))\n",
    "        #[30] min reg time\n",
    "        temp.append(min(pay_time))\n",
    "        #[31] reg time span\n",
    "        temp.append(max(pay_time) - min(pay_time))\n",
    "        #[32] reg time mean\n",
    "        temp.append(pay_time.quantile(0.5))\n",
    "        #[33] reg time std\n",
    "        temp.append(np.std(pay_time))\n",
    "    \n",
    "    ret[each] = temp\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string2list(in_str):\n",
    "    ret = in_str.split('],[')\n",
    "    temp = []\n",
    "    for each in ret:\n",
    "        temp.append(each.strip('[').strip(']').split(','))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
